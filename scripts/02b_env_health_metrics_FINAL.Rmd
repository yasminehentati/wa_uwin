---
title: "02b_env_health_metrics_FINAL"
output: html_document
date: "2024-09-18"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Packages
```{r}

library(pacman)

p_load(dplyr,sp,raster,here,sf,mapview,rgdal,remotes,tidycensus,
       tidyr,terra,spatialEco,readr,ggfortify,exactextractr,spatstat,
       spdep,landscapemetrics,tmap,viridis,gghighlight,purrr,plainview,tidyterra,
       crsuggest,janitor)

# note that mapview is somewhat demanding, consider skipping
# mapview functions if your computer is slow  - these are just used to check 
# that shapefiles look correct 

```


## Read in and clean environmental health data 
```{r}
# read in env health data 

lead <- read_csv(here("data", "env_health_data", "Lead_Risk_from_Housing.csv"), 
                 name_repair = janitor::make_clean_names) %>% 
  select(census_tract, number_units_w_lead_hazard_estimated)
          #        col_select = c("County_Name", "Census_Tract", "Units_Lead", 
              #                 "Total_Units", "Pct_Units_Lead", "IBL_Rank"))
colnames(lead)

pm25 <- read_csv(here("data", "env_health_data", "PM2.5_Concentration.csv"), 
                 name_repair = janitor::make_clean_names) %>% 
  select(census_tract, count)
             #    col_names=c("County_Name", "YearGroup", "Census_Tract", "PM2.5_Count", "IBL_Rank"))

prox_haz_waste <- read_csv(here("data", "env_health_data", 
                                "Proximity_to_Hazardous_Waste_Treatment_Storage_and_Disposal_Facilities_(TSDFs).csv"), 
                           name_repair = janitor::make_clean_names) %>% 
  select(census_tract, average_ptsdf)
# col_names=c("County_Name", "Census_Tract", "Average_PTSDF", "IBL_Rank"))

prox_heavy_traff <- read_csv(here("data", "env_health_data", 
                                          "Proximity_to_Heavy_Traffic_Roadways.csv"), 
                                  name_repair = janitor::make_clean_names) %>% 
  select(census_tract, proximity_to_heavy_traffic_roadways)
                             #     col_names=c("County_Name", "Census_Tract", "Prox_Heavy_Traffic_Roadways", "IBL_Rank"))

prox_superfund <- read_csv(here("data", "env_health_data", "Proximity_to_National_Priorities_List_Facilities_(Superfund_Sites).csv"), 
                           name_repair = janitor::make_clean_names) %>% 
select( census_tract, average_pnpl)
# col_names=c("County_Name", "Census_Tract", "Average_PNPL", "IBL_Rank"))

toxic_release <- read_csv(here("data", "env_health_data", "Toxic_Releases_from_Facilities_(RSEI_Model).csv"), 
                          name_repair = janitor::make_clean_names) %>% 
  select(census_tract, average_rsei_concentrations)
# col_names=c("County_Name", "Census_Tract", "Average_RSEI_Concentrations", "IBL_Rank"))

wastewater <- read_csv(here("data", "env_health_data", "Wastewater_Discharge.csv"), 
                       name_repair = janitor::make_clean_names)  %>% 
  select(census_tract, average_pwdis)
# col_names=c("County_Name", "Census_Tract", "Average_PWDIS", "IBL_Rank"))

diesel_nox <- read_csv(here("data", "env_health_data", "Diesel_Emission_Levels_of_NOx_(Annual_Tons_Km2).csv"), 
                       name_repair = janitor::make_clean_names) %>% 
  select(census_tract, annual_tons_km2)
# col_names=c("County_Name", "Census_Tract", "Annual Tons Km2 Diesel NOx", "IBL_Rank"))

prox_rmp <- read_csv(here("data", "env_health_data", "Proximity_to_Risk_Management_Plan_(RMP)_Facililties.csv"), 
                     name_repair = janitor::make_clean_names) %>% 
  select(census_tract, average_prmp)
#  col_names=c("County_Name", "Census_Tract", "Average PRMP", "IBL_Rank"))

ozone <- read_csv(here("data", "env_health_data", "Ozone_Concentration.csv"), 
                  name_repair = janitor::make_clean_names) %>% 
  select(census_tract, average_ozone_concentration_ppm_km2)

# col_names=c("County_Name", "Census_Tract", "Average Ozone Concentration ppb km2", "IBL_Rank"))

# combine all into one DF 

# first list data frames together 
listdf <- list(lead, pm25, prox_haz_waste, prox_heavy_traff, prox_superfund,
               toxic_release, wastewater, diesel_nox, prox_rmp, ozone)

# remove ranking number 
#listdf <- lapply(listdf, subset, select = -county_name)

# combine data by GEOID 
envdat <- listdf %>% reduce(full_join, by = "census_tract")

# remove first row
envdat <- tail(envdat, -1)

#get only our counties of interest 
envdat <- envdat %>% dplyr::filter(substr(census_tract, 1, 5) 
                                 %in% c("53033", "53053", # king pierce 
                                         "53061", "53035", # snoho kitsap
                                        "53029", "53057")) # island skagit


# rename census tract to geoid and rename pollution columns to be a bit more concise 
envdat <- rename(envdat,c("GEOID" = "census_tract", 
                          "units_with_lead" = "number_units_w_lead_hazard_estimated",
                          "pm2.5_count" = "count",
                          "avg_ptsdf" = "average_ptsdf",
                          "prox_heavy_traffic" = "proximity_to_heavy_traffic_roadways",
                          "avg_pnpl" = "average_pnpl",
                          "avg_rsei" = "average_rsei_concentrations",
                         "avg_pwdis" = "average_pwdis",
                         "diesel_tons_km2" = "annual_tons_km2",
                         "avg_prmp" = "average_prmp",
                          "avg_ozone" = "average_ozone_concentration_ppm_km2")) 

## create polygons for env health data based on geoids 
# we'll use the income data to do this 

# read in income data
tractsKP <- st_read(here("data", "income_maps", "seatac_urban_med_income.shp")) %>% 
  st_transform(crs = "EPSG:32610")

st_crs(tractsKP)
# merge env health data to polygons 

# join based on GEOID
envdatSP <- merge(tractsKP, envdat, by = "GEOID") %>% 
  st_as_sf()

st_crs(envdatSP)


# write shapefile for our env health metrics 
st_write(envdatSP, here("data", "env_health_data", "env_health_all_KP.shp"), append = FALSE)

##### read in camera data so we have our points for the buffers 
# same proj 

sites <- st_read("data/cameras/points_wa.shp") 
                  

envdatSP

# plot one of our variables to check 
mapview(list(sites, envdatSP),
        zcol = list(NULL, "avg_pnpl"))

ggplot(data = envdatSP) +
  geom_sf(aes(fill = units_with_lead)) +
  theme_minimal() +
  labs(fill = "number units with lead hazard")

```


## 1. Calculate site values for lead risk 
```{r}

# rasterize the data 
template <- rast(ext(envdatSP), resolution=100, crs="EPSG:32610")
lead_rast <- terra::rasterize(vect(envdatSP), template, field = "units_with_lead")

# initialize new col for data
lead_dat <- st_drop_geometry(sites) %>%
  dplyr::mutate(units_lead = NA_real_)

# set buffer radius
buffer_radius <- 500

Sys.time()

############
for (i in 1:nrow(sites)) {
  pt <- sites[i, ]  # iterate through sites 

  # create buffer
  buff <- vect(st_transform(pt, crs(lead_rast)))
  buff_terra <- terra::buffer(buff, width = buffer_radius) 
  
  # need to give it a bit extra extent 
  buff_ext <- ext(buff_terra) 
  buff_ext <- ext(c(
    xmin = buff_ext$xmin - 1000,
    xmax = buff_ext$xmax + 1000,
    ymin = buff_ext$ymin - 1000,
    ymax = buff_ext$ymax + 1000))
  buff_terra <- crop(buff_terra, buff_ext)
  
  # crop and mask the raster 
  lead_rast_cropped <- crop(lead_rast, buff_terra)
  lead_rast_masked <- terra::mask(lead_rast_cropped, buff_terra)
  
  # Extract values within the buffer and compute the mean
  extracted_value <- exact_extract(lead_rast_masked, st_as_sf(buff_terra), fun = "mean", 
                                   weights = "area")
  lead_dat$units_lead[i] <- extracted_value
  
  # Convert the raster to a data frame for ggplot2
  lead_rast_df <- as.data.frame(lead_rast_masked, xy = TRUE)
  
  # Plot the results
  
  p <- ggplot() +
    geom_raster(lead_rast_cropped, mapping = aes(x = x, y = y, fill = units_with_lead)) +
  #   scale_fill_manual() +
    geom_sf(data = st_as_sf(buff_terra), fill = NA, color = "red", size = 1) +
    geom_sf(data = sites[i,], color = "blue", size = 3) +
    coord_sf(crs = st_crs(lead_rast)) +
    labs(title = paste("Site:", i),
         x = "Easting (m)", y = "Northing (m)",
         fill = "Raster Value") +
    theme_minimal()
  
  # Save the plot
  # ggsave(filename = paste0("plots/plot_site_", i, ".png"), plot = p, width = 8, height = 6)
  print(p) 
  Sys.sleep(2)
   # Uncomment to plot
    
  print(i)
}

Sys.time()

# Check the final results
print(lead_dat)

# save 
write_csv(lead_dat, here("data", "covariates", "lead_dat.csv"))
```

## 1. Calculate site values for PM2.5
```{r}

# rasterize the data
template <- rast(ext(envdatSP), resolution=100, crs="EPSG:32610")
rast <- terra::rasterize(vect(envdatSP), template, field = "pm2.5_count")

# initialize new col for data
pm2.5_dat <- st_drop_geometry(sites) %>%
  dplyr::mutate(pm2.5_count = NA_real_)

# set buffer radius
buffer_radius <- 500

Sys.time()

for (i in 1:nrow(sites)) {
  pt <- sites[i, ]  # iterate through sites 
  
  # create buffer
  buff <- vect(st_transform(pt, crs(rast)))
  buff_terra <- terra::buffer(buff, width = buffer_radius) 
  
  # need to give it a bit extra extent 
  buff_ext <- ext(buff_terra) 
  buff_ext <- ext(c(
    xmin = buff_ext$xmin - 1000,
    xmax = buff_ext$xmax + 1000,
    ymin = buff_ext$ymin - 1000,
    ymax = buff_ext$ymax + 1000))
  buff_terra <- crop(buff_terra, buff_ext)
  
  # crop and mask the raster 
  rast_cropped <- crop(rast, buff_terra)
  rast_masked <- terra::mask(rast_cropped, buff_terra)
  
  # xxtract values within the buffer, calculate mean
  extracted_value <- exact_extract(rast_masked, st_as_sf(buff_terra), fun = "mean", 
                                   weights = "area")
  pm2.5_dat$pm2.5_count[i] <- extracted_value
  
  
  # Plot the results
  
  p <- ggplot() +
    geom_raster(rast_cropped, mapping = aes(x = x, y = y, fill = pm2.5_count)) +
    #   scale_fill_manual() +
    geom_sf(data = st_as_sf(buff_terra), fill = NA, color = "red", size = 1) +
    geom_sf(data = sites[i,], color = "blue", size = 3) +
    coord_sf(crs = st_crs(rast), datum = st_crs(rast)) +
    labs(title = paste("Site:", i),
         x = "Easting (m)", y = "Northing (m)",
         fill = "Raster Value") +
    theme_minimal()
  
  # Save the plot
  # ggsave(filename = paste0("plots/plot_site_", i, ".png"), plot = p, width = 8, height = 6)
  print(p) 
  Sys.sleep(2)
  print(i)
}

Sys.time()

# Check the final results
print(pm2.5_dat)

# save
write_csv(pm2.5_dat, here("data", "covariates", "pm2.5_dat.csv"))
```

## 3. Calculate site values for proximity to hazardous waste (PTSDF)
```{r}

# rasterize the data
template <- rast(ext(envdatSP), resolution=100, crs="EPSG:32610")
rast <- terra::rasterize(vect(envdatSP), template, field = "avg_ptsdf")

# initialize new col for data
ptsdf_dat <- st_drop_geometry(sites) %>%
  dplyr::mutate(avg_ptsdf = NA_real_)

# set buffer radius
buffer_radius <- 500

Sys.time()

for (i in 1:nrow(sites)) {
  pt <- sites[i, ]  # iterate through sites 
  
  # create buffer
  buff <- vect(st_transform(pt, crs(rast)))
  buff_terra <- terra::buffer(buff, width = buffer_radius) 
  
  # need to give it a bit extra extent 
  buff_ext <- ext(buff_terra) 
  buff_ext <- ext(c(
    xmin = buff_ext$xmin - 1000,
    xmax = buff_ext$xmax + 1000,
    ymin = buff_ext$ymin - 1000,
    ymax = buff_ext$ymax + 1000))
  buff_terra <- crop(buff_terra, buff_ext)
  
  # crop and mask the raster 
  rast_cropped <- crop(rast, buff_terra)
  rast_masked <- terra::mask(rast_cropped, buff_terra)
  
  # xxtract values within the buffer, calculate mean
  extracted_value <- exact_extract(rast_masked, st_as_sf(buff_terra), fun = "mean", 
                                   weights = "area")
  ptsdf_dat$avg_ptsdf[i] <- extracted_value
  
  
  # Plot the results
  
  p <- ggplot() +
    geom_raster(rast_cropped, mapping = aes(x = x, y = y, fill = avg_ptsdf)) +
    #   scale_fill_manual() +
    geom_sf(data = st_as_sf(buff_terra), fill = NA, color = "red", size = 1) +
    geom_sf(data = sites[i,], color = "blue", size = 3) +
    coord_sf(crs = st_crs(rast), datum = st_crs(rast)) +
    labs(title = paste("Site:", i),
         x = "Easting (m)", y = "Northing (m)",
         fill = "Raster Value") +
    theme_minimal()
  
  # Save the plot
  # ggsave(filename = paste0("plots/plot_site_", i, ".png"), plot = p, width = 8, height = 6)
  print(p) 
  Sys.sleep(2)
  print(i)
}

Sys.time()

# Check the final results
print(ptsdf_dat)

# write 
write_csv(ptsdf_dat, here("data", "covariates", "ptsdf_dat.csv"))
```

## 4. Calculate site values for proximity to heavy traffic roadways
```{r}
head(envdatSP)
# rasterize the data
template <- rast(ext(envdatSP), resolution=100, crs="EPSG:32610")
rast <- terra::rasterize(vect(envdatSP), template, field = "prox_heavy_traffic")

# initialize new col for data
traffic_dat <- st_drop_geometry(sites) %>%
  dplyr::mutate(prox_heavy_traffic = NA_real_)

# set buffer radius
buffer_radius <- 500

Sys.time()

for (i in 1:nrow(sites)) {
  pt <- sites[i, ]  # iterate through sites 
  
  # create buffer
  buff <- vect(st_transform(pt, crs(rast)))
  buff_terra <- terra::buffer(buff, width = buffer_radius) 
  
  # need to give it a bit extra extent 
  buff_ext <- ext(buff_terra) 
  buff_ext <- ext(c(
    xmin = buff_ext$xmin - 1000,
    xmax = buff_ext$xmax + 1000,
    ymin = buff_ext$ymin - 1000,
    ymax = buff_ext$ymax + 1000))
  buff_terra <- crop(buff_terra, buff_ext)
  
  # crop and mask the raster 
  rast_cropped <- crop(rast, buff_terra)
  rast_masked <- terra::mask(rast_cropped, buff_terra)
  
  # xxtract values within the buffer, calculate mean
  extracted_value <- exact_extract(rast_masked, st_as_sf(buff_terra), fun = "mean", 
                                   weights = "area")
  traffic_dat$prox_heavy_traffic[i] <- extracted_value
  
  
  # Plot the results
  
  p <- ggplot() +
    geom_raster(rast_cropped, mapping = aes(x = x, y = y, fill = prox_heavy_traffic)) +
    #   scale_fill_manual() +
    geom_sf(data = st_as_sf(buff_terra), fill = NA, color = "red", size = 1) +
    geom_sf(data = sites[i,], color = "blue", size = 3) +
    coord_sf(crs = st_crs(rast), datum = st_crs(rast)) +
    labs(title = paste("Site:", i),
         x = "Easting (m)", y = "Northing (m)",
         fill = "Raster Value") +
    theme_minimal()
  
  # Save the plot
  # ggsave(filename = paste0("plots/plot_site_", i, ".png"), plot = p, width = 8, height = 6)
  print(p) 
  Sys.sleep(2)
  print(i)
}

Sys.time()

# Check the final results
print(traffic_dat)


# save 
write_csv(traffic_dat, here("data", "covariates", "traffic_dat.csv"))
```

## 5. Calculate site values for proximity to Superfund sites 
```{r}
head(envdatSP)
# rasterize the data
template <- rast(ext(envdatSP), resolution=100, crs="EPSG:32610")
rast <- terra::rasterize(vect(envdatSP), template, field = "avg_pnpl")

# initialize new col for data
pnpl_dat <- st_drop_geometry(sites) %>%
  dplyr::mutate(avg_pnpl = NA_real_)

# set buffer radius
buffer_radius <- 500

Sys.time()

for (i in 1:nrow(sites)) {
  pt <- sites[i, ]  # iterate through sites 
  
  # create buffer
  buff <- vect(st_transform(pt, crs(rast)))
  buff_terra <- terra::buffer(buff, width = buffer_radius) 
  
  # need to give it a bit extra extent 
  buff_ext <- ext(buff_terra) 
  buff_ext <- ext(c(
    xmin = buff_ext$xmin - 1000,
    xmax = buff_ext$xmax + 1000,
    ymin = buff_ext$ymin - 1000,
    ymax = buff_ext$ymax + 1000))
  buff_terra <- crop(buff_terra, buff_ext)
  
  # crop and mask the raster 
  rast_cropped <- crop(rast, buff_terra)
  rast_masked <- terra::mask(rast_cropped, buff_terra)
  
  # xxtract values within the buffer, calculate mean
  extracted_value <- exact_extract(rast_masked, st_as_sf(buff_terra), fun = "mean", 
                                   weights = "area")
  pnpl_dat$avg_pnpl[i] <- extracted_value
  
  
  # Plot the results
  
  p <- ggplot() +
    geom_raster(rast_cropped, mapping = aes(x = x, y = y, fill = avg_pnpl)) +
    #   scale_fill_manual() +
    geom_sf(data = st_as_sf(buff_terra), fill = NA, color = "red", size = 1) +
    geom_sf(data = sites[i,], color = "blue", size = 3) +
    coord_sf(crs = st_crs(rast), datum = st_crs(rast)) +
    labs(title = paste("Site:", i),
         x = "Easting (m)", y = "Northing (m)",
         fill = "Raster Value") +
    theme_minimal()
  
  # Save the plot
  # ggsave(filename = paste0("plots/plot_site_", i, ".png"), plot = p, width = 8, height = 6)
  print(p) 
  Sys.sleep(2)
  print(i)
}

Sys.time()

# Check the final results
print(pnpl_dat)

# save 
write_csv(pnpl_dat, here("data", "covariates", "pnpl_dat.csv"))
```

## 6. Calculate site values for proximity to toxic release facilities
```{r}
head(envdatSP)
# rasterize the data
template <- rast(ext(envdatSP), resolution=100, crs="EPSG:32610")
rast <- terra::rasterize(vect(envdatSP), template, field = "avg_rsei")

# initialize new col for data
rsei_dat <- st_drop_geometry(sites) %>%
  dplyr::mutate(avg_rsei = NA_real_)

# set buffer radius
buffer_radius <- 500

Sys.time()

for (i in 1:nrow(sites)) {
  pt <- sites[i, ]  # iterate through sites 
  
  # create buffer
  buff <- vect(st_transform(pt, crs(rast)))
  buff_terra <- terra::buffer(buff, width = buffer_radius) 
  
  # need to give it a bit extra extent 
  buff_ext <- ext(buff_terra) 
  buff_ext <- ext(c(
    xmin = buff_ext$xmin - 1000,
    xmax = buff_ext$xmax + 1000,
    ymin = buff_ext$ymin - 1000,
    ymax = buff_ext$ymax + 1000))
  buff_terra <- crop(buff_terra, buff_ext)
  
  # crop and mask the raster 
  rast_cropped <- crop(rast, buff_terra)
  rast_masked <- terra::mask(rast_cropped, buff_terra)
  
  # xxtract values within the buffer, calculate mean
  extracted_value <- exact_extract(rast_masked, st_as_sf(buff_terra), fun = "mean", 
                                   weights = "area")
  rsei_dat$avg_rsei[i] <- extracted_value
  
  
  # Plot the results
  
  p <- ggplot() +
    geom_raster(rast_cropped, mapping = aes(x = x, y = y, fill = avg_rsei)) +
    #   scale_fill_manual() +
    geom_sf(data = st_as_sf(buff_terra), fill = NA, color = "red", size = 1) +
    geom_sf(data = sites[i,], color = "blue", size = 3) +
    coord_sf(crs = st_crs(rast), datum = st_crs(rast)) +
    labs(title = paste("Site:", i),
         x = "Easting (m)", y = "Northing (m)",
         fill = "Raster Value") +
    theme_minimal()
  
  # Save the plot
  # ggsave(filename = paste0("plots/plot_site_", i, ".png"), plot = p, width = 8, height = 6)
  print(p) 
  Sys.sleep(2)
  print(i)
}

Sys.time()

# Check the final results
print(rsei_dat)

# save 
write_csv(rsei_dat, here("data", "covariates", "rsei_dat.csv"))
```

## 7. Calculate site values for wastewater discharge 
```{r}
head(envdatSP)
# rasterize the data
template <- rast(ext(envdatSP), resolution=100, crs="EPSG:32610")
rast <- terra::rasterize(vect(envdatSP), template, field = "avg_pwdis")

# initialize new col for data
pwdis_dat <- st_drop_geometry(sites) %>%
  dplyr::mutate(avg_pwdis = NA_real_)

# set buffer radius
buffer_radius <- 500

Sys.time()

for (i in 1:nrow(sites)) {
  pt <- sites[i, ]  # iterate through sites 
  
  # create buffer
  buff <- vect(st_transform(pt, crs(rast)))
  buff_terra <- terra::buffer(buff, width = buffer_radius) 
  
  # need to give it a bit extra extent 
  buff_ext <- ext(buff_terra) 
  buff_ext <- ext(c(
    xmin = buff_ext$xmin - 1000,
    xmax = buff_ext$xmax + 1000,
    ymin = buff_ext$ymin - 1000,
    ymax = buff_ext$ymax + 1000))
  buff_terra <- crop(buff_terra, buff_ext)
  
  # crop and mask the raster 
  rast_cropped <- crop(rast, buff_terra)
  rast_masked <- terra::mask(rast_cropped, buff_terra)
  
  # xxtract values within the buffer, calculate mean
  extracted_value <- exact_extract(rast_masked, st_as_sf(buff_terra), fun = "mean", 
                                   weights = "area")
  pwdis_dat$avg_pwdis[i] <- extracted_value
  
  
  # Plot the results
  
  p <- ggplot() +
    geom_raster(rast_cropped, mapping = aes(x = x, y = y, fill = avg_pwdis)) +
    #   scale_fill_manual() +
    geom_sf(data = st_as_sf(buff_terra), fill = NA, color = "red", size = 1) +
    geom_sf(data = sites[i,], color = "blue", size = 3) +
    coord_sf(crs = st_crs(rast), datum = st_crs(rast)) +
    labs(title = paste("Site:", i),
         x = "Easting (m)", y = "Northing (m)",
         fill = "Raster Value") +
    theme_minimal()
  
  # Save the plot
  # ggsave(filename = paste0("plots/plot_site_", i, ".png"), plot = p, width = 8, height = 6)
  print(p) 
  Sys.sleep(2)
  print(i)
}

Sys.time()

# Check the final results
print(pwdis_dat)

write_csv(pwdis_dat, here("data", "covariates", "pwdis_dat.csv"))
```

## 8. Calculate site values for diesel pollution 
```{r}
head(envdatSP)
# rasterize the data
template <- rast(ext(envdatSP), resolution=100, crs="EPSG:32610")
rast <- terra::rasterize(vect(envdatSP), template, field = "avg_diesel_tons_km2")

# initialize new col for data
diesel_dat <- st_drop_geometry(sites) %>%
  dplyr::mutate(avg_diesel_tons_km2 = NA_real_)

# set buffer radius
buffer_radius <- 500

Sys.time()

for (i in 1:nrow(sites)) {
  pt <- sites[i, ]  # iterate through sites 
  
  # create buffer
  buff <- vect(st_transform(pt, crs(rast)))
  buff_terra <- terra::buffer(buff, width = buffer_radius) 
  
  # need to give it a bit extra extent 
  buff_ext <- ext(buff_terra) 
  buff_ext <- ext(c(
    xmin = buff_ext$xmin - 1000,
    xmax = buff_ext$xmax + 1000,
    ymin = buff_ext$ymin - 1000,
    ymax = buff_ext$ymax + 1000))
  buff_terra <- crop(buff_terra, buff_ext)
  
  # crop and mask the raster 
  rast_cropped <- crop(rast, buff_terra)
  rast_masked <- terra::mask(rast_cropped, buff_terra)
  
  # xxtract values within the buffer, calculate mean
  extracted_value <- exact_extract(rast_masked, st_as_sf(buff_terra), fun = "mean", 
                                   weights = "area")
  diesel_dat$avg_diesel_tons_km2[i] <- extracted_value
  
  
  # Plot the results
  
  p <- ggplot() +
    geom_raster(rast_cropped, mapping = aes(x = x, y = y, fill = avg_diesel_tons_km2)) +
    #   scale_fill_manual() +
    geom_sf(data = st_as_sf(buff_terra), fill = NA, color = "red", size = 1) +
    geom_sf(data = sites[i,], color = "blue", size = 3) +
    coord_sf(crs = st_crs(rast), datum = st_crs(rast)) +
    labs(title = paste("Site:", i),
         x = "Easting (m)", y = "Northing (m)",
         fill = "Raster Value") +
    theme_minimal()
  
  # Save the plot
  # ggsave(filename = paste0("plots/plot_site_", i, ".png"), plot = p, width = 8, height = 6)
  print(p) 
  Sys.sleep(2)
  print(i)
}

Sys.time()

# Check the final results
print(diesel_dat)

#save 
write_csv(diesel_dat, here("data", "covariates", "diesel_dat.csv"))
```

## 9. Calculate site values for proximity to PRMP sites 
```{r}
head(envdatSP)
# rasterize the data
template <- rast(ext(envdatSP), resolution=100, crs="EPSG:32610")
rast <- terra::rasterize(vect(envdatSP), template, field = "avg_prmp")

# initialize new col for data
prmp_dat <- st_drop_geometry(sites) %>%
  dplyr::mutate(avg_prmp = NA_real_)

# set buffer radius
buffer_radius <- 500

Sys.time()

for (i in 1:nrow(sites)) {
  pt <- sites[i, ]  # iterate through sites 
  
  # create buffer
  buff <- vect(st_transform(pt, crs(rast)))
  buff_terra <- terra::buffer(buff, width = buffer_radius) 
  
  # need to give it a bit extra extent 
  buff_ext <- ext(buff_terra) 
  buff_ext <- ext(c(
    xmin = buff_ext$xmin - 1000,
    xmax = buff_ext$xmax + 1000,
    ymin = buff_ext$ymin - 1000,
    ymax = buff_ext$ymax + 1000))
  buff_terra <- crop(buff_terra, buff_ext)
  
  # crop and mask the raster 
  rast_cropped <- crop(rast, buff_terra)
  rast_masked <- terra::mask(rast_cropped, buff_terra)
  
  # xxtract values within the buffer, calculate mean
  extracted_value <- exact_extract(rast_masked, st_as_sf(buff_terra), fun = "mean", 
                                   weights = "area")
  prmp_dat$avg_prmp[i] <- extracted_value
  
  
  # Plot the results
  
  p <- ggplot() +
    geom_raster(rast_cropped, mapping = aes(x = x, y = y, fill = avg_prmp)) +
    #   scale_fill_manual() +
    geom_sf(data = st_as_sf(buff_terra), fill = NA, color = "red", size = 1) +
    geom_sf(data = sites[i,], color = "blue", size = 3) +
    coord_sf(crs = st_crs(rast), datum = st_crs(rast)) +
    labs(title = paste("Site:", i),
         x = "Easting (m)", y = "Northing (m)",
         fill = "Raster Value") +
    theme_minimal()
  
  # Save the plot
  # ggsave(filename = paste0("plots/plot_site_", i, ".png"), plot = p, width = 8, height = 6)
  print(p) 
  Sys.sleep(2)
  print(i)
}

Sys.time()

# Check the final results
print(prmp_dat)

# save
write_csv(prmp_dat, here("data", "covariates", "prmp_dat.csv"))
```

## 10. Calculate site values for ozone 
```{r}
head(envdatSP)
# rasterize the data
template <- rast(ext(envdatSP), resolution=100, crs="EPSG:32610")
rast <- terra::rasterize(vect(envdatSP), template, field = "avg_ozone")

# initialize new col for data
ozone_dat <- st_drop_geometry(sites) %>%
  dplyr::mutate(avg_ozone = NA_real_)

# set buffer radius
buffer_radius <- 500

Sys.time()

for (i in 1:nrow(sites)) {
  pt <- sites[i, ]  # iterate through sites 
  
  # create buffer
  buff <- vect(st_transform(pt, crs(rast)))
  buff_terra <- terra::buffer(buff, width = buffer_radius) 
  
  # need to give it a bit extra extent 
  buff_ext <- ext(buff_terra) 
  buff_ext <- ext(c(
    xmin = buff_ext$xmin - 1000,
    xmax = buff_ext$xmax + 1000,
    ymin = buff_ext$ymin - 1000,
    ymax = buff_ext$ymax + 1000))
  buff_terra <- crop(buff_terra, buff_ext)
  
  # crop and mask the raster 
  rast_cropped <- crop(rast, buff_terra)
  rast_masked <- terra::mask(rast_cropped, buff_terra)
  
  # xxtract values within the buffer, calculate mean
  extracted_value <- exact_extract(rast_masked, st_as_sf(buff_terra), fun = "mean", 
                                   weights = "area")
  ozone_dat$avg_ozone[i] <- extracted_value
  
  
  # Plot the results
  
  p <- ggplot() +
    geom_raster(rast_cropped, mapping = aes(x = x, y = y, fill = avg_ozone)) +
    #   scale_fill_manual() +
    geom_sf(data = st_as_sf(buff_terra), fill = NA, color = "red", size = 1) +
    geom_sf(data = sites[i,], color = "blue", size = 3) +
    coord_sf(crs = st_crs(rast), datum = st_crs(rast)) +
    labs(title = paste("Site:", i),
         x = "Easting (m)", y = "Northing (m)",
         fill = "Raster Value") +
    theme_minimal()
  
  # Save the plot
  # ggsave(filename = paste0("plots/plot_site_", i, ".png"), plot = p, width = 8, height = 6)
  print(p) 
  Sys.sleep(2)
  print(i)
}

Sys.time()

# Check the final results
print(ozone_dat)


# save
write_csv(ozone_dat, here("data", "covariates", "ozone_dat.csv"))
```



## Combine all env health variables and create burden percentiles 
```{r}

# below code adapted from cesar estien

### bring in data 


#now lets make one big pollutant dataframe for the zonal stats

# first list data frames together 
listdfs <- list(lead_dat, pm2.5_dat, ptsdf_dat, traffic_dat, 
                pnpl_dat, rsei_dat, pwdis_dat, 
                diesel_dat, prmp_dat, ozone_dat)

# combine data by city/site
pollutants <- listdfs %>% reduce(full_join, by = c("city", "site"))


#create percentile for each metric. we'll use this to create the pollution burden score later
pollutants <- pollutants %>%
  mutate(LEAD_PCT = ntile(pollutants$lead_dat, 100)) %>%
  mutate(PM25_PCT = ntile(pollutants$pm2.5_dat, 100)) %>%
  mutate(HAZARD_PCT = ntile(pollutants$ptsdf_dat, 100)) %>%
  mutate(TRAFFIC_PCT = ntile(pollutants$traffic_dat, 100)) %>%
  mutate(PNPL_PCT = ntile(pollutants$pnpl_dat, 100)) %>% 
  mutate(TOXIC_PCT = ntile(pollutants$rsei_dat, 100)) %>% 
  mutate(PWDIS_PCT = ntile(pollutants$pwdis_dat, 100)) %>% 
  mutate(OZONE_PCT = ntile(pollutants$ozone_dat, 100)) %>% 
  mutate(PRMP_PCT = ntile(pollutants$prmp_dat, 100)) 


#first we need to average the exposures (pm25, diesel, toxic releases (RESI), ozone)
# we'll skip prox traffic because it's too similar to the others

# and environmental risks (superfund sites, risk management plan facilities, hazardous waste facilities, lead risk)
# we'll skip wastewater bc we don't have enough data 

pollutants <- pollutants %>%
  mutate(EXPOSURE = (PM25_PCT + DIESEL_PCT +  TOXIC_PCT + OZONE_PCT) / 4) %>%
  mutate(RISK = (LEAD_PCT + PRMP_PCT + PTSDF_PCT + PNPL_PCT) / 4) %>% 
  
  #then we need to get the indicator for both. exposure stays the same but effects is given HALF weight. so multiply the RISK COLUMN by .5
  
  mutate(RISK = RISK * 0.5) %>% 
  #BURDEN: now we sum the EXPOSURE AND RISK COLUMN (Pollution Burden is calculated as the average of its two component scores, with the Environmental RISK component halfweighted.)
  
  mutate(BURDEN = RISK + EXPOSURE) %>% 
  #THEN WE divide it by the weights. + (1 / .5)
  mutate(BURDEN = BURDEN/1.5) %>% 
  #then we want to scale it (0-10) so find the max and divide it by that number  
  mutate(BURDEN_SCALED = (((BURDEN /max(BURDEN)) * 10))) %>% 
  #then we can bin it as a percentile to help with scale
  mutate(BURDEN_PERCENTILE = ntile(BURDEN_SCALED, 100))



# now calculate city-specific pollution burden 

## seattle 

# combine data by city/site and remove tawa 
pollutants_sewa <- listdfs %>% reduce(full_join, by = c("city", "site")) %>%
  filter(city == "sewa")

#create percentile for each metric. we'll use this to create the pollution burden score later
pollutants_sewa <- pollutants_sewa %>%
  mutate(SEWA_LEAD_PCT = ntile(pollutants_sewa$lead_dat, 100)) %>%
  mutate(SEWA_PM25_PCT = ntile(pollutants_sewa$pm2.5_dat, 100)) %>%
  mutate(SEWA_HAZARD_PCT = ntile(pollutants_sewa$ptsdf_dat, 100)) %>%
  mutate(SEWA_TRAFFIC_PCT = ntile(pollutants_sewa$traffic_dat, 100)) %>%
  mutate(SEWA_PNPL_PCT = ntile(pollutants_sewa$pnpl_dat, 100)) %>% 
  mutate(SEWA_TOXIC_PCT = ntile(pollutants_sewa$rsei_dat, 100)) %>% 
  mutate(SEWA_PWDIS_PCT = ntile(pollutants_sewa$pwdis_dat, 100)) %>% 
  mutate(SEWA_OZONE_PCT = ntile(pollutants_sewa$ozone_dat, 100)) %>% 
  mutate(SEWA_PRMP_PCT = ntile(pollutants_sewa$prmp_dat, 100)) 


#first we need to average the exposures (pm25, diesel, toxic releases (RESI), ozone)
# we'll skip prox traffic because it's too similar to the others

# and environmental risks (superfund sites, risk management plan facilities, hazardous waste facilities, lead risk)
# we'll skip wastewater bc we don't have enough data 

pollutants_sewa <- pollutants_sewa %>%
  mutate(SEWA_EXPOSURE = (SEWA_PM25_PCT + SEWA_DIESEL_PCT +  SEWA_TOXIC_PCT + SEWA_OZONE_PCT) / 4) %>%
  mutate(SEWA_RISK = (SEWA_LEAD_PCT + SEWA_PRMP_PCT + SEWA_PTSDF_PCT + SEWA_PNPL_PCT) / 4) %>% 
  
  #then we need to get the indicator for both. exposure stays the same but effects is given HALF weight. so multiply the RISK COLUMN by .5
  
  mutate(SEWA_RISK = SEWA_RISK * 0.5) %>% 
  #BURDEN: now we sum the EXPOSURE AND RISK COLUMN (Pollution Burden is calculated as the average of its two component scores, with the Environmental RISK component halfweighted.)
  
  mutate(SEWA_BURDEN = SEWA_RISK+ SEWA_EXPOSURE) %>% 
  #THEN WE divide it by the weights. + (1 / .5)
  mutate(SEWA_BURDEN = SEWA_BURDEN/1.5) %>% 
  #then we want to scale it (0-10) so find the max and divide it by that number  
  mutate(SEWA_BURDEN_SCALED = (((SEWA_BURDEN /max(SEWA_BURDEN)) * 10))) %>% 
  #then we can bin it as a percentile to help with scale
  mutate(SEWA_BURDEN_PERCENTILE = ntile(SEWA_BURDEN_SCALED, 100))


## tacoma 
# combine data by city/site and remove tawa 
pollutants_tawa <- listdfs %>% reduce(full_join, by = c("city", "site")) %>%
  filter(city == "tawa")

#create percentile for each metric. we'll use this to create the pollution burden score later
pollutants_tawa <- pollutants_tawa %>%
  mutate(TAWA_LEAD_PCT = ntile(pollutants_tawa$lead_dat, 100)) %>%
  mutate(TAWA_PM25_PCT = ntile(pollutants_tawa$pm2.5_dat, 100)) %>%
  mutate(TAWA_HAZARD_PCT = ntile(pollutants_tawa$ptsdf_dat, 100)) %>%
  mutate(TAWA_TRAFFIC_PCT = ntile(pollutants_tawa$traffic_dat, 100)) %>%
  mutate(TAWA_PNPL_PCT = ntile(pollutants_tawa$pnpl_dat, 100)) %>% 
  mutate(TAWA_TOXIC_PCT = ntile(pollutants_tawa$rsei_dat, 100)) %>% 
  mutate(TAWA_PWDIS_PCT = ntile(pollutants_tawa$pwdis_dat, 100)) %>% 
  mutate(TAWA_OZONE_PCT = ntile(pollutants_tawa$ozone_dat, 100)) %>% 
  mutate(TAWA_PRMP_PCT = ntile(pollutants_tawa$prmp_dat, 100)) 


#first we need to average the exposures (pm25, diesel, toxic releases (RESI), ozone)
# we'll skip prox traffic because it's too similar to the others

# and environmental risks (superfund sites, risk management plan facilities, hazardous waste facilities, lead risk)
# we'll skip wastewater bc we don't have enough data 

pollutants_tawa <- pollutants_tawa %>%
  mutate(TAWA_EXPOSURE = (TAWA_PM25_PCT + TAWA_DIESEL_PCT +  TAWA_ROXIC_PCT + TAWA_OZONE_PCT) / 4) %>%
  mutate(TAWA_RISK = (TAWA_LEAD_PCT + TAWA_PRMP_PCT + TAWA_PTSDF_PCT + TAWA_PNPL_PCT) / 4) %>% 
  
  #then we need to get the indicator for both. exposure stays the same but effects is given HALF weight. so multiply the RISK COLUMN by .5
  
  mutate(TAWA_RISK = TAWA_RISK * 0.5) %>% 
  #BURDEN: now we sum the EXPOSURE AND RISK COLUMN (Pollution Burden is calculated as the average of its two component scores, with the Environmental RISK component halfweighted.)
  
  mutate(TAWA_BURDEN = TAWA_RISK+ TAWA_EXPOSURE) %>% 
  #THEN WE divide it by the weights. + (1 / .5)
  mutate(TAWA_BURDEN = TAWA_BURDEN/1.5) %>% 
  #then we want to scale it (0-10) so find the max and divide it by that number  
  mutate(TAWA_BURDEN_SCALED = (((TAWA_BURDEN /max(TAWA_BURDEN)) * 10))) %>% 
  #then we can bin it as a percentile to help with scale
  mutate(TAWA_BURDEN_PERCENTILE = ntile(TAWA_BURDEN_SCALED, 100))



# bring in urbanizatioon covariate data and merge 

urbcovs <- read_csv(here("data", "covariates", "sitecov_1000m_sewatawa_allsites.csv"))

sewa_urbcovs <- urb_covs %>% filter(city == "sewa")
tawa_urbcovs <- urb_covs %>% filter(city == "tawa")

all_covs <- left_join(urbcovs, pollutants, by = c("city", "site"))
sewa_all_covs <- left_join(sewa_urbcovs, pollutants_sewa, by = c("site"))
tawa_all_covs <- left_join(tawa_urbcovs, pollutants_tawa, by = c("site"))

# write 
write_csv(all_covs, here("data", "covariates", "ALL_ENV_URB_SITES_1000m.csv"))
write_csv(sewa_all_covs, here("data", "covariates", "SEWA_ALL_ENV_URB_SITES_1000m.csv"))
write_csv(tawa_all_covs, here("data", "covariates", "TAWA_ALL_ENV_URB_SITES_1000m.csv"))


# bind count data to covariate data 

counts <- read_csv(here("data", "wa_counts.csv"))
sewa_counts <- counts %>% filter(city == "sewa")
tawa_counts <- counts %>% filter(city == "tawa")

all_data <- left_join(counts, all_covs, by = c("city", "site"))
sewa_all_data <- left_join(sewa_counts, all_covs, by = c("site"))
tawa_all_data <- left_join(sewa_counts, all_covs, by = c("site"))

write_csv(all_data, here("data", "covariates", "COUNTS_ALL_ENV_URB_SITES_1000m.csv"))
write_csv(sewa_all_data, here("data", "covariates", "SEWA_COUNTS_ALL_ENV_URB_SITES_1000m.csv"))
write_csv(tawa_all_data, here("data", "covariates", "TAWA_COUNTS_ALL_ENV_URB_SITES_1000m.csv"))

```

## extra code 
```{r}

########## 

## do not run -- only if adding a covariate in 02a
library(here)
library(readr)
install.packages("conflicted")
library(conflicted)
urbcovs <- read_csv(here::here("data", "covariates", "sitecov_1000m_sewatawa_allsites.csv"))
colnames(urbcovs)
urbcovs <- urbcovs %>% dplyr::select(c(city, site, pop_density:prop_veg_avg))

all_covs <- read_csv(here::here("data", "covariates", "ALL_ENV_URB_SITES_1000m.csv")) %>%
  dplyr::select(-c(pop_density.x:prop_veg_avg.y))
colnames(all_covs)
all_covs <- left_join(all_covs, urbcovs, by = c("city", "site"))
colnames(all_covs)
write_csv(all_covs, here::here("data", "covariates", "ALL_ENV_URB_SITES_1000m.csv"))

counts <- read_csv(here::here("data", "wa_counts.csv"))

all_data <- left_join(counts, all_covs, by = c("city", "site"))

write_csv(all_data, here::here("data", "covariates", "COUNTS_ALL_ENV_URB_SITES_1000m.csv"))
```

